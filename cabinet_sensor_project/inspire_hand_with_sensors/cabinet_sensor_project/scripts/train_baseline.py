#!/usr/bin/env python3
# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
ğŸ¤– Cabinet RL Training - BASELINE Version (No Sensors)

åŸºçº¿ç‰ˆæœ¬çš„æœºæ¢°è‡‚å¼€æŠ½å±‰å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œä¸ä½¿ç”¨æ¥è§¦ä¼ æ„Ÿå™¨ã€‚
ä½œä¸ºå¯¹ç…§ç»„ï¼Œä¸cabinet_rl_with_sensors_new.pyè¿›è¡Œä¸¥æ ¼çš„æ§åˆ¶å˜é‡å¯¹æ¯”ã€‚

Usage:
    ./isaaclab.sh -p cabinet_rl_BASELINE.py --num_envs 64 --headless
"""

import argparse
import os
import torch
from datetime import datetime

from isaaclab.app import AppLauncher

# add argparse arguments
parser = argparse.ArgumentParser(description="Train RL agent for cabinet opening task - BASELINE (No Sensors)")
parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
parser.add_argument("--video_interval", type=int, default=2000, help="Interval between video recordings (in steps).")
parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
parser.add_argument("--max_iterations", type=int, default=None, help="Maximum number of training iterations")
parser.add_argument(
    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
)

# append AppLauncher cli args
AppLauncher.add_app_launcher_args(parser)
# parse the arguments
args_cli = parser.parse_args()

# launch omniverse app
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

"""Rest everything follows."""

import gymnasium as gym

from rsl_rl.runners import OnPolicyRunner

import isaaclab_tasks  # noqa: F401
from isaaclab_tasks.manager_based.manipulation.cabinet import mdp
from isaaclab_tasks.manager_based.manipulation.cabinet.cabinet_env_cfg import CabinetEnvCfg
from isaaclab_tasks.utils.parse_cfg import parse_env_cfg
from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlVecEnvWrapper
from isaaclab.utils.dict import print_dict
from isaaclab.utils.io import dump_yaml, dump_pickle


def main():
    """Main function."""
    
    # parse configuration - ä¸ä¼ æ„Ÿå™¨ç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„æ–¹æ³•
    env_cfg: CabinetEnvCfg = parse_env_cfg(
        "Isaac-Open-Drawer-Franka-IK-Abs-v0",
        device="cuda:0",
        num_envs=args_cli.num_envs if args_cli.num_envs is not None else 8,
        use_fabric=not args_cli.disable_fabric,
    )
    
    # ğŸš« ä¸æ·»åŠ ä¼ æ„Ÿå™¨é…ç½® - è¿™æ˜¯ä¸ä¼ æ„Ÿå™¨ç‰ˆæœ¬çš„å”¯ä¸€åŒºåˆ«
    # æ³¨æ„ï¼šè¿™é‡Œæ•…æ„ä¸è°ƒç”¨patch_env_cfg_with_contact_sensors(env_cfg)
    print("[BASELINE] ğŸ” No sensors configured - using standard observations only")
    
    # ğŸš« ä¸æ·»åŠ ä¼ æ„Ÿå™¨æ•°æ®åˆ°è§‚æµ‹ä¸­ - ä¿æŒæ ‡å‡†è§‚æµ‹ç©ºé—´
    # æ³¨æ„ï¼šè¿™é‡Œæ•…æ„ä¸æ·»åŠ  env_cfg.observations.policy.contact_forces
    
    # Load agent configuration - ä¸ä¼ æ„Ÿå™¨ç‰ˆæœ¬å®Œå…¨ç›¸åŒ
    from isaaclab_tasks.manager_based.manipulation.cabinet.config.franka.agents.rsl_rl_ppo_cfg import CabinetPPORunnerCfg
    agent_cfg: RslRlOnPolicyRunnerCfg = CabinetPPORunnerCfg()

    # Override max_iterations if provided
    if args_cli.max_iterations is not None:
        agent_cfg.max_iterations = args_cli.max_iterations
        print(f"[INFO] Using command line max_iterations: {args_cli.max_iterations}")

    # specify directory for logging experiments
    log_root_path = os.path.join("logs", "rsl_rl", "cabinet_baseline")
    log_root_path = os.path.abspath(log_root_path)
    print(f"[INFO] Logging experiment in directory: {log_root_path}")
    
    # specify directory for logging runs
    log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    if args_cli.seed is not None:
        log_dir += f"_seed{args_cli.seed}"
    log_dir = os.path.join(log_root_path, log_dir)

    # create isaac environment - ä¸ä¼ æ„Ÿå™¨ç‰ˆæœ¬ç›¸åŒçš„ä»»åŠ¡å
    env = gym.make("Isaac-Open-Drawer-Franka-IK-Abs-v0", cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
    
    # wrap for video recording
    if args_cli.video:
        video_kwargs = {
            "video_folder": os.path.join(log_dir, "videos", "train"),
            "episode_trigger": lambda step: step % args_cli.video_interval == 0,
            "step_trigger": None,
            "video_length": args_cli.video_length,
            "disable_logger": True,
        }
        print("[INFO] Recording videos during training.")
        print_dict(video_kwargs, nesting=4)
        env = gym.wrappers.RecordVideo(env, **video_kwargs)

    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env)

    print(f"[INFO] Environment observation space: {env.num_obs}")
    print(f"[INFO] Environment action space: {env.num_actions}")
    print(f"[INFO] Environment episode length: {env.max_episode_length}")
    
    # ğŸ” éªŒè¯è§‚æµ‹ç©ºé—´ - åŸºçº¿ç‰ˆæœ¬åº”è¯¥æ˜¯æ ‡å‡†ç»´åº¦
    print(f"\nğŸ” [VERIFICATION] Baseline observation space analysis:")
    expected_baseline_obs = 31  # åŸºç¡€è§‚æµ‹ç»´åº¦ (without sensors)
    print(f"  - Expected baseline obs dimensions: {expected_baseline_obs}")
    print(f"  - Actual obs dimensions: {env.num_obs}")
    
    if env.num_obs == expected_baseline_obs:
        print(f"  âœ… SUCCESS: Baseline using standard observations (no sensors)")
    else:
        print(f"  âš ï¸  WARNING: Unexpected observation dimensions for baseline")
    
    # ğŸ” æµ‹è¯•è§‚æµ‹è®¿é—®
    print(f"\nğŸ” [VERIFICATION] Testing baseline observations:")
    try:
        test_obs = env.get_observations()
        print(f"  - Successfully got observations: {test_obs[0].shape}")
        print(f"  - Baseline obs (env 0, first 10 dims): {test_obs[0][:10].cpu().numpy()}")
        
    except Exception as e:
        print(f"  âŒ Error accessing observations: {e}")

    # set seed of the environment
    seed_value = args_cli.seed if args_cli.seed is not None else 42
    env.seed(seed_value)

    # create runner from rsl-rl
    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)
    # write git state to logs
    runner.add_git_repo_to_log(__file__)

    # dump the configuration into log-directory
    dump_yaml(os.path.join(log_dir, "params", "env.yaml"), env_cfg)
    dump_yaml(os.path.join(log_dir, "params", "agent.yaml"), agent_cfg)
    dump_pickle(os.path.join(log_dir, "params", "env.pkl"), env_cfg)
    dump_pickle(os.path.join(log_dir, "params", "agent.pkl"), agent_cfg)

    print("[INFO] ğŸš€ Starting BASELINE training (no sensors)...")
    print(f"[INFO] Total training steps: {agent_cfg.max_iterations}")
    print(f"[INFO] Environment episodes per step: {env_cfg.scene.num_envs}")
    print(f"[INFO] Using standard observations only (NO sensor data)")

    # run training
    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)

    print("[INFO] âœ… BASELINE training completed!")

    # save the final model
    save_path = os.path.join(log_dir, "model_{}.pt".format(runner.current_learning_iteration))
    runner.save(save_path)
    print(f"[INFO] Saved baseline model to {save_path}")

    # close the simulator
    env.close()


if __name__ == "__main__":
    # run the main function
    main()
    # close sim app
    simulation_app.close() 