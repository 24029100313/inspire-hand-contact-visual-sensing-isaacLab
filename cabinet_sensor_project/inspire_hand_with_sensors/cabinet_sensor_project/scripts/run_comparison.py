#!/usr/bin/env python3
"""
ğŸ§ª ä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒè„šæœ¬

è‡ªåŠ¨è¿è¡Œä¼ æ„Ÿå™¨ç‰ˆæœ¬å’ŒåŸºçº¿ç‰ˆæœ¬çš„è®­ç»ƒï¼Œå¹¶è¿›è¡Œå…¨é¢çš„æ€§èƒ½å¯¹æ¯”åˆ†æã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è‡ªåŠ¨è¿è¡Œå¤šç§é…ç½®çš„è®­ç»ƒå®éªŒ
2. å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦å’Œæ€§èƒ½
3. æ”¶é›†å’Œè§£æè®­ç»ƒæ•°æ®
4. ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š
5. å¯è§†åŒ–è®­ç»ƒæ›²çº¿å’Œæ€§èƒ½æŒ‡æ ‡
6. è‡ªåŠ¨æ¸…ç†Isaac Simè¿›ç¨‹ï¼Œé˜²æ­¢èµ„æºå ç”¨

æ–°åŠŸèƒ½ï¼š
- å®æ—¶è®­ç»ƒçŠ¶æ€ç›‘æ§ï¼ˆæ¯Næ¬¡è¿­ä»£æ˜¾ç¤ºè¿›åº¦ï¼‰
- æ™ºèƒ½è¿›ç¨‹ç®¡ç†å’ŒGPUå†…å­˜ç›‘æ§
- å¯é…ç½®çš„æ¸…ç†ç­–ç•¥å’Œç›‘æ§é—´éš”

ä¾èµ–åŒ…ï¼š
    pip install psutil

Usage Examples:
    # åŸºæœ¬ä½¿ç”¨ - æ¯5æ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
    python run_sensor_comparison_experiment.py --num_seeds 3 --max_iterations 2000 --num_envs 64
    
    # è‡ªå®šä¹‰çŠ¶æ€æ˜¾ç¤ºé—´éš”ï¼ˆæ¯10æ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡ï¼‰
    python run_sensor_comparison_experiment.py --num_seeds 3 --status_interval 10
    
    # ç¦ç”¨å®æ—¶ç›‘æ§ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼
    python run_sensor_comparison_experiment.py --num_seeds 3 --disable_realtime_monitoring
    
    # å¿«é€Ÿæµ‹è¯•ï¼ˆå°‘é‡è¿­ä»£ï¼Œé¢‘ç¹çŠ¶æ€æ›´æ–°ï¼‰
    python run_sensor_comparison_experiment.py --num_seeds 1 --max_iterations 100 --status_interval 5
    
    # é•¿æ—¶é—´è®­ç»ƒï¼ˆè‡ªå®šä¹‰è¶…æ—¶å’Œæ¸…ç†é—´éš”ï¼‰
    python run_sensor_comparison_experiment.py --num_seeds 5 --max_iterations 5000 --timeout 14400 --cleanup_wait 120
"""

import argparse
import os
import subprocess
import time
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
import yaml
import glob
from typing import Dict, List, Tuple
import shutil
import signal
import re
import select

# æ£€æŸ¥å¹¶å¯¼å…¥psutil
try:
    import psutil
except ImportError:
    print("âŒ [ERROR] psutil package is required for process management")
    print("ğŸ“¦ This package is needed to properly clean up Isaac Sim processes")
    
    user_input = input("ğŸ¤” Would you like to install psutil automatically? (y/n): ").lower().strip()
    if user_input in ['y', 'yes']:
        print("ğŸ“¥ [INSTALL] Installing psutil...")
        try:
            subprocess.run([
                "pip", "install", "psutil"
            ], check=True)
            print("âœ… [SUCCESS] psutil installed successfully!")
            import psutil
        except subprocess.CalledProcessError:
            print("âŒ [ERROR] Failed to install psutil automatically")
            print("ğŸ”§ Please install manually using: pip install psutil")
            exit(1)
        except Exception as e:
            print(f"âŒ [ERROR] Installation failed: {e}")
            print("ğŸ”§ Please install manually using: pip install psutil")
            exit(1)
    else:
        print("ğŸ”§ Please install psutil manually using: pip install psutil")
        print("   Or if you're using conda: conda install psutil")
        exit(1)


class SensorComparisonExperiment:
    """ä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_dir = Path(f"experiments/sensor_comparison_{self.timestamp}")
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # å®éªŒé…ç½®
        self.configs = {
            'with_sensors': {
                'script': 'cabinet_rl_with_sensors_new.py',
                'name': 'With Sensors',
                'color': '#2E86AB',
                'expected_obs_dim': 43
            },
            'baseline': {
                'script': 'cabinet_rl_BASELINE.py', 
                'name': 'Baseline (No Sensors)',
                'color': '#A23B72',
                'expected_obs_dim': 31
            }
        }
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        self.training_logs = {}
        
        print(f"ğŸ§ª [EXPERIMENT] Sensor Comparison Experiment")
        print(f"ğŸ“ Experiment directory: {self.experiment_dir}")
        print(f"ğŸŒ± Seeds: {list(range(args.num_seeds))}")
        print(f"ğŸ”„ Max iterations: {args.max_iterations}")
        print(f"ğŸŒ Environments: {args.num_envs}")
    
    def cleanup_isaac_processes(self):
        """æ¸…ç†Isaac Simç›¸å…³è¿›ç¨‹"""
        if self.args.disable_cleanup:
            print("âš ï¸ [SKIP] Process cleanup disabled by user")
            return
            
        print("ğŸ§¹ [CLEANUP] Cleaning up Isaac Sim processes...")
        
        # Isaac Simç›¸å…³çš„è¿›ç¨‹åç§°
        isaac_process_names = [
            'isaac-sim',
            'python.sh',
            'kit',
            'omni.isaac.sim',
            'nvidia-isaac-sim',
            'libwayland-egl.so'
        ]
        
        # é¦–å…ˆå°è¯•ä¼˜é›…åœ°ç»“æŸè¿›ç¨‹
        killed_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['cmdline'] is None:
                    continue
                    
                cmdline_str = ' '.join(proc.info['cmdline']).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯Isaacç›¸å…³è¿›ç¨‹
                if any(name in cmdline_str for name in isaac_process_names):
                    print(f"ğŸ” Found Isaac process: PID={proc.info['pid']}, CMD={cmdline_str[:100]}...")
                    try:
                        proc.terminate()  # å‘é€SIGTERM
                        killed_processes.append(proc.info['pid'])
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
                        
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                continue
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        if killed_processes:
            print(f"â±ï¸ [CLEANUP] Waiting for {len(killed_processes)} processes to terminate...")
            time.sleep(5)
            
            # å¼ºåˆ¶æ€æ­»ä»ç„¶å­˜åœ¨çš„è¿›ç¨‹
            for pid in killed_processes:
                try:
                    proc = psutil.Process(pid)
                    if proc.is_running():
                        print(f"ğŸ’€ [CLEANUP] Force killing process {pid}")
                        proc.kill()
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        
        # ä½¿ç”¨ç³»ç»Ÿå‘½ä»¤ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        try:
            subprocess.run(["pkill", "-f", "isaac"], capture_output=True, timeout=10)
            subprocess.run(["pkill", "-f", "omni"], capture_output=True, timeout=10)
            subprocess.run(["pkill", "-f", "kit"], capture_output=True, timeout=10)
        except subprocess.TimeoutExpired:
            print("âš ï¸ [CLEANUP] pkill command timed out")
        except Exception as e:
            print(f"âš ï¸ [CLEANUP] pkill failed: {e}")
        
        print("âœ… [CLEANUP] Process cleanup completed")
    
    def check_gpu_memory(self):
        """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=memory.used,memory.total", "--format=csv,noheader,nounits"],
                capture_output=True, text=True, timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for i, line in enumerate(lines):
                    used, total = line.split(', ')
                    used_mb = int(used)
                    total_mb = int(total)
                    usage_percent = (used_mb / total_mb) * 100
                    print(f"ğŸ–¥ï¸ [GPU {i}] Memory: {used_mb}MB / {total_mb}MB ({usage_percent:.1f}%)")
                    
                    if usage_percent > self.args.gpu_memory_threshold:
                        print(f"âš ï¸ [WARNING] GPU {i} memory usage is high ({usage_percent:.1f}%)")
                        return False
                return True
            else:
                print("âš ï¸ [WARNING] Could not check GPU memory")
                return True
                
        except Exception as e:
            print(f"âš ï¸ [WARNING] GPU memory check failed: {e}")
            return True
    
    def wait_for_system_ready(self):
        """ç­‰å¾…ç³»ç»Ÿå‡†å¤‡å°±ç»ª"""
        print("â³ [WAIT] Waiting for system to be ready...")
        
        # åŸºæœ¬ç­‰å¾…æ—¶é—´
        time.sleep(10)
        
        # æ£€æŸ¥GPUå†…å­˜ï¼Œå¦‚æœå ç”¨è¿‡é«˜åˆ™ç»§ç»­ç­‰å¾…
        max_wait_attempts = 6  # æœ€å¤šç­‰å¾…60ç§’
        for attempt in range(max_wait_attempts):
            if self.check_gpu_memory():
                break
            else:
                print(f"â³ [WAIT] GPU memory still high, waiting... (attempt {attempt+1}/{max_wait_attempts})")
                time.sleep(10)
        
        print("âœ… [READY] System ready for next training")
    
    def parse_training_output_line(self, line: str) -> Dict:
        """è§£æè®­ç»ƒè¾“å‡ºè¡Œï¼Œæå–å…³é”®æŒ‡æ ‡"""
        metrics = {}
        
        # å¸¸è§çš„è®­ç»ƒæŒ‡æ ‡æ¨¡å¼
        patterns = {
            'iteration': r'(?:Iteration|iter|step)[\s:=]+(\d+)',
            'reward': r'(?:reward|episode_reward|mean_reward)[\s:=]+([-+]?\d*\.?\d+)',
            'loss': r'(?:loss|policy_loss|value_loss)[\s:=]+([-+]?\d*\.?\d+)',
            'episode_length': r'(?:episode_length|ep_len)[\s:=]+([-+]?\d*\.?\d+)',
            'success_rate': r'(?:success_rate|success)[\s:=]+([-+]?\d*\.?\d+)',
            'lr': r'(?:learning_rate|lr)[\s:=]+([-+]?\d*\.?\d+(?:e[-+]?\d+)?)',
            'fps': r'(?:fps|FPS)[\s:=]+([-+]?\d*\.?\d+)',
            'time_elapsed': r'(?:time|elapsed)[\s:=]+([-+]?\d*\.?\d+)'
        }
        
        line_lower = line.lower()
        for metric_name, pattern in patterns.items():
            match = re.search(pattern, line_lower)
            if match:
                try:
                    metrics[metric_name] = float(match.group(1))
                except ValueError:
                    pass
        
        return metrics
    
    def display_training_status(self, config_name: str, seed: int, iteration: int, metrics: Dict, start_time: float):
        """æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€"""
        config = self.configs[config_name]
        elapsed_time = time.time() - start_time
        
        print(f"\nğŸ“Š [STATUS] {config['name']} (seed={seed}) - Iteration {iteration}")
        print(f"â±ï¸  Elapsed: {elapsed_time:.1f}s")
        
        if metrics:
            # æ˜¾ç¤ºå¯ç”¨çš„æŒ‡æ ‡
            if 'reward' in metrics:
                print(f"ğŸ¯ Reward: {metrics['reward']:.3f}")
            if 'loss' in metrics:
                print(f"ğŸ“‰ Loss: {metrics['loss']:.6f}")
            if 'episode_length' in metrics:
                print(f"ğŸ“ Episode Length: {metrics['episode_length']:.1f}")
            if 'success_rate' in metrics:
                print(f"âœ… Success Rate: {metrics['success_rate']:.1%}")
            if 'lr' in metrics:
                print(f"ğŸ“š Learning Rate: {metrics['lr']:.2e}")
            if 'fps' in metrics:
                print(f"ğŸš€ FPS: {metrics['fps']:.1f}")
        else:
            print("ğŸ“‹ Waiting for metrics...")
        
        # è¿›åº¦æ¡
        if self.args.max_iterations > 0:
            progress = iteration / self.args.max_iterations
            bar_length = 30
            filled_length = int(bar_length * progress)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            print(f"ğŸ“ˆ Progress: |{bar}| {progress:.1%} ({iteration}/{self.args.max_iterations})")

    def run_single_training(self, config_name: str, seed: int) -> Dict:
        """è¿è¡Œå•ä¸ªè®­ç»ƒå®éªŒ"""
        config = self.configs[config_name]
        script_name = config['script']
        
        print(f"\nğŸš€ [TRAINING] Starting {config['name']} (seed={seed})")
        print(f"ğŸ“„ Script: {script_name}")
        
        # è®­ç»ƒå‰æ¸…ç†
        self.cleanup_isaac_processes()
        self.wait_for_system_ready()
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "./isaaclab.sh", "-p", script_name,
            "--num_envs", str(self.args.num_envs),
            "--max_iterations", str(self.args.max_iterations),
            "--seed", str(seed),
            "--headless"
        ]
        
        if self.args.disable_fabric:
            cmd.append("--disable_fabric")
        
        print(f"ğŸ’» Command: {' '.join(cmd)}")
        
        # é€‰æ‹©è¿è¡Œæ¨¡å¼
        if getattr(self.args, 'disable_realtime_monitoring', False):
            return self._run_training_traditional_mode(config_name, seed, cmd)
        else:
            return self._run_training_realtime_mode(config_name, seed, cmd)
    
    def _run_training_traditional_mode(self, config_name: str, seed: int, cmd: list) -> Dict:
        """ä¼ ç»Ÿæ¨¡å¼ï¼šç­‰å¾…è®­ç»ƒå®Œæˆåå¤„ç†è¾“å‡º"""
        config = self.configs[config_name]
        start_time = time.time()
        
        print("ğŸ“Š [TRADITIONAL MODE] Training without real-time monitoring...")
        
        try:
            result = subprocess.run(
                cmd,
                cwd="/home/larry/NVIDIA_DEV/isaac_grasp_ws/IsaacLab",
                capture_output=True,
                text=True,
                timeout=getattr(self.args, 'timeout', 3600)
            )
            
            end_time = time.time()
            training_time = end_time - start_time
            
            if result.returncode == 0:
                print(f"âœ… [SUCCESS] {config['name']} (seed={seed}) completed in {training_time:.1f}s")
                success = True
                error_msg = None
            else:
                print(f"âŒ [ERROR] {config['name']} (seed={seed}) failed")
                print(f"stderr: {result.stderr[-500:]}")
                success = False
                error_msg = result.stderr
                
        except subprocess.TimeoutExpired:
            print(f"â° [TIMEOUT] {config['name']} (seed={seed}) timed out")
            success = False
            error_msg = "Training timed out"
            training_time = getattr(self.args, 'timeout', 3600)
            result = None
            
        except Exception as e:
            print(f"ğŸ’¥ [EXCEPTION] {config['name']} (seed={seed}) failed: {e}")
            success = False
            error_msg = str(e)
            training_time = time.time() - start_time
            result = None
        
        # è®­ç»ƒåæ¸…ç†
        print("ğŸ§¹ [POST-TRAINING] Cleaning up processes...")
        self.cleanup_isaac_processes()
        
        return {
            'config_name': config_name,
            'seed': seed,
            'success': success,
            'training_time': training_time,
            'error_msg': error_msg,
            'stdout': result.stdout if result else "",
            'stderr': result.stderr if result else error_msg or "",
            'final_iteration': None,
            'final_metrics': {}
        }
    
    def _run_training_realtime_mode(self, config_name: str, seed: int, cmd: list) -> Dict:
        """å®æ—¶ç›‘æ§æ¨¡å¼ï¼šå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦"""
        config = self.configs[config_name]
        start_time = time.time()
        
        # å®æ—¶ç›‘æ§å˜é‡
        current_iteration = 0
        status_interval = getattr(self.args, 'status_interval', 5)
        last_displayed_iteration = -status_interval  # ç¡®ä¿ç¬¬ä¸€æ¬¡è¿­ä»£å°±æ˜¾ç¤º
        latest_metrics = {}
        output_lines = []
        error_lines = []
        
        try:
            process = subprocess.Popen(
                cmd,
                cwd="/home/larry/NVIDIA_DEV/isaac_grasp_ws/IsaacLab",
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            print(f"ğŸ”„ [REALTIME MODE] Training started (PID: {process.pid})")
            print(f"ğŸ“Š Status updates every {status_interval} iterations")
            
            # å®æ—¶è¯»å–è¾“å‡º
            while True:
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç»“æŸ
                if process.poll() is not None:
                    break
                
                # è¯»å–stdout
                if process.stdout:
                    try:
                        ready, _, _ = select.select([process.stdout], [], [], 0.1)
                        if ready:
                            line = process.stdout.readline()
                            if line:
                                output_lines.append(line)
                                
                                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰è¾“å‡ºè¡Œ
                                if "iter" in line.lower() or "episode" in line.lower() or "reward" in line.lower():
                                    print(f"ğŸ” [DEBUG] Training output: {line.strip()}")
                                
                                # è§£æå½“å‰è¡Œçš„æŒ‡æ ‡
                                line_metrics = self.parse_training_output_line(line)
                                if line_metrics:
                                    latest_metrics.update(line_metrics)
                                    print(f"ğŸ“Š [PARSED] Found metrics: {line_metrics}")
                                    
                                    # æ›´æ–°è¿­ä»£è®¡æ•°
                                    if 'iteration' in line_metrics:
                                        current_iteration = int(line_metrics['iteration'])
                                
                                # æ¯Næ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
                                if (current_iteration > 0 and 
                                    current_iteration % status_interval == 0 and 
                                    current_iteration > last_displayed_iteration):
                                    
                                    self.display_training_status(
                                        config_name, seed, current_iteration, 
                                        latest_metrics, start_time
                                    )
                                    last_displayed_iteration = current_iteration
                    except (OSError, ValueError):
                        # å¤„ç†selectæˆ–è¯»å–é”™è¯¯
                        pass
                
                # è¯»å–stderr
                if process.stderr:
                    try:
                        ready, _, _ = select.select([process.stderr], [], [], 0.1)
                        if ready:
                            error_line = process.stderr.readline()
                            if error_line:
                                error_lines.append(error_line)
                    except (OSError, ValueError):
                        pass
                
                # æ£€æŸ¥è¶…æ—¶
                elapsed = time.time() - start_time
                timeout = getattr(self.args, 'timeout', 3600)
                if elapsed > timeout:
                    print(f"\nâ° [TIMEOUT] Training exceeded {timeout}s, terminating...")
                    process.terminate()
                    time.sleep(5)
                    if process.poll() is None:
                        process.kill()
                    break
            
            # è¯»å–å‰©ä½™è¾“å‡º
            try:
                remaining_stdout, remaining_stderr = process.communicate(timeout=30)
                if remaining_stdout:
                    output_lines.extend(remaining_stdout.splitlines())
                if remaining_stderr:
                    error_lines.extend(remaining_stderr.splitlines())
            except subprocess.TimeoutExpired:
                print("âš ï¸ [WARNING] Timeout while reading final output")
            
            end_time = time.time()
            training_time = end_time - start_time
            
            # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
            return_code = process.returncode
            if return_code == 0:
                print(f"\nâœ… [SUCCESS] {config['name']} (seed={seed}) completed in {training_time:.1f}s")
                print(f"ğŸ¯ Final iteration: {current_iteration}")
                if latest_metrics:
                    print(f"ğŸ“ˆ Final metrics: {latest_metrics}")
                success = True
                error_msg = None
            else:
                print(f"\nâŒ [ERROR] {config['name']} (seed={seed}) failed (exit code: {return_code})")
                if error_lines:
                    print(f"stderr: {''.join(error_lines[-10:])}")
                success = False
                error_msg = '\n'.join(error_lines)
            
        except Exception as e:
            print(f"\nğŸ’¥ [EXCEPTION] {config['name']} (seed={seed}) failed: {e}")
            success = False
            error_msg = str(e)
            training_time = time.time() - start_time
            output_lines = []
            error_lines = [str(e)]
        
        # è®­ç»ƒåæ¸…ç†
        print("ğŸ§¹ [POST-TRAINING] Cleaning up processes...")
        self.cleanup_isaac_processes()
        
        return {
            'config_name': config_name,
            'seed': seed,
            'success': success,
            'training_time': training_time,
            'error_msg': error_msg,
            'stdout': '\n'.join(output_lines),
            'stderr': '\n'.join(error_lines),
            'final_iteration': current_iteration,
            'final_metrics': latest_metrics
        }
    
    def parse_training_logs(self, config_name: str, seed: int) -> Dict:
        """è§£æè®­ç»ƒæ—¥å¿—è·å–æ€§èƒ½æŒ‡æ ‡"""
        config = self.configs[config_name]
        
        # æŸ¥æ‰¾å¯¹åº”çš„æ—¥å¿—ç›®å½•
        if config_name == 'with_sensors':
            log_pattern = f"logs/rsl_rl/cabinet_with_sensors_new/*/seed{seed}"
        else:
            log_pattern = f"logs/rsl_rl/cabinet_baseline/*/seed{seed}"
        
        log_dirs = glob.glob(log_pattern)
        if not log_dirs:
            print(f"âš ï¸ [WARNING] No log directory found for {config['name']} seed={seed}")
            return {}
        
        log_dir = Path(log_dirs[-1])  # ä½¿ç”¨æœ€æ–°çš„æ—¥å¿—ç›®å½•
        
        # è§£ætensorboardæ—¥å¿—æˆ–å…¶ä»–æ—¥å¿—æ–‡ä»¶
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…çš„æ—¥å¿—æ ¼å¼è¿›è¡Œè§£æ
        parsed_data = {
            'final_reward': None,
            'convergence_iteration': None,
            'success_rate': None,
            'episode_length': None
        }
        
        # å°è¯•è§£æsummaries.jsonæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        summaries_file = log_dir / "summaries.json"
        if summaries_file.exists():
            try:
                with open(summaries_file, 'r') as f:
                    summaries = json.load(f)
                    # æå–æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
                    if 'Episode_Reward' in summaries:
                        parsed_data['final_reward'] = summaries['Episode_Reward'][-1] if summaries['Episode_Reward'] else None
            except Exception as e:
                print(f"ğŸ“„ [LOG] Error parsing summaries.json: {e}")
        
        return parsed_data
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        print(f"\nğŸ¯ [EXPERIMENT] Starting full comparison experiment")
        
        total_runs = len(self.configs) * self.args.num_seeds
        current_run = 0
        
        for config_name in self.configs.keys():
            self.results[config_name] = []
            
            for seed in range(self.args.num_seeds):
                current_run += 1
                print(f"\nğŸ“Š [PROGRESS] Run {current_run}/{total_runs}")
                
                # è¿è¡Œè®­ç»ƒ
                result = self.run_single_training(config_name, seed)
                
                # è§£ææ—¥å¿—
                if result['success']:
                    log_data = self.parse_training_logs(config_name, seed)
                    result.update(log_data)
                
                # ä¿å­˜ç»“æœ
                self.results[config_name].append(result)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self.save_intermediate_results()
                
                # è®­ç»ƒé—´éš” - æ›´é•¿çš„ç­‰å¾…æ—¶é—´å’Œå½»åº•æ¸…ç†
                if current_run < total_runs:
                    print("ğŸ§¹ [INTERVAL] Performing thorough cleanup between training runs...")
                    
                    # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰å¯èƒ½çš„Isaacè¿›ç¨‹
                    self.cleanup_isaac_processes()
                    
                    # é¢å¤–ç­‰å¾…ï¼Œè®©GPUå†…å­˜å®Œå…¨é‡Šæ”¾
                    half_wait = self.args.cleanup_wait // 2
                    print(f"â±ï¸ [INTERVAL] Waiting {half_wait}s for complete resource cleanup...")
                    time.sleep(half_wait)
                    
                    # å†æ¬¡æ£€æŸ¥å’Œæ¸…ç†
                    self.cleanup_isaac_processes()
                    self.wait_for_system_ready()
                    
                    print(f"â±ï¸ [INTERVAL] Final {half_wait}s wait before next training...")
                    time.sleep(half_wait)
        
        print(f"\nğŸ‰ [COMPLETE] All experiments completed!")
    
    def save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        results_file = self.experiment_dir / "intermediate_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
    
    def analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“ˆ [ANALYSIS] Analyzing experiment results...")
        
        # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
        all_data = []
        for config_name, results in self.results.items():
            for result in results:
                row = {
                    'config': config_name,
                    'config_name': self.configs[config_name]['name'],
                    'seed': result['seed'],
                    'success': result['success'],
                    'training_time': result['training_time'],
                    'final_reward': result.get('final_reward'),
                    'convergence_iteration': result.get('convergence_iteration'),
                    'success_rate': result.get('success_rate'),
                    'episode_length': result.get('episode_length')
                }
                all_data.append(row)
        
        df = pd.DataFrame(all_data)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats = {}
        for config_name in self.configs.keys():
            config_data = df[df['config'] == config_name]
            successful_runs = config_data[config_data['success'] == True]
            
            stats[config_name] = {
                'total_runs': len(config_data),
                'successful_runs': len(successful_runs),
                'success_rate': len(successful_runs) / len(config_data),
                'avg_training_time': successful_runs['training_time'].mean() if len(successful_runs) > 0 else None,
                'std_training_time': successful_runs['training_time'].std() if len(successful_runs) > 0 else None,
                'avg_final_reward': successful_runs['final_reward'].mean() if len(successful_runs) > 0 else None,
                'std_final_reward': successful_runs['final_reward'].std() if len(successful_runs) > 0 else None
            }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_file = self.experiment_dir / "analysis_results.json"
        with open(analysis_file, 'w') as f:
            json.dump(stats, f, indent=2, default=str)
        
        df.to_csv(self.experiment_dir / "experiment_data.csv", index=False)
        
        return df, stats
    
    def generate_visualizations(self, df: pd.DataFrame, stats: Dict):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print(f"ğŸ“Š [VISUALIZATION] Generating comparison plots...")
        
        # è®¾ç½®ç»˜å›¾é£æ ¼ - å…¼å®¹ä¸åŒç‰ˆæœ¬çš„seaborn
        try:
            plt.style.use('seaborn-v0_8-whitegrid')
        except OSError:
            try:
                plt.style.use('seaborn-whitegrid')
            except OSError:
                plt.style.use('default')
                print("âš ï¸ [WARNING] Using default matplotlib style (seaborn not available)")
        
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Sensor vs Baseline Comparison', fontsize=16, fontweight='bold')
        
        # 1. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        successful_df = df[df['success'] == True]
        if len(successful_df) > 0:
            sns.boxplot(data=successful_df, x='config_name', y='training_time', ax=axes[0,0])
            axes[0,0].set_title('Training Time Comparison')
            axes[0,0].set_ylabel('Training Time (seconds)')
            axes[0,0].tick_params(axis='x', rotation=45)
        
        # 2. æˆåŠŸç‡å¯¹æ¯”
        success_rates = []
        config_names = []
        for config_name, stat in stats.items():
            success_rates.append(stat['success_rate'] * 100)
            config_names.append(self.configs[config_name]['name'])
        
        bars = axes[0,1].bar(config_names, success_rates, 
                            color=[self.configs[k]['color'] for k in stats.keys()])
        axes[0,1].set_title('Training Success Rate')
        axes[0,1].set_ylabel('Success Rate (%)')
        axes[0,1].set_ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, success_rates):
            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                          f'{rate:.1f}%', ha='center', va='bottom')
        
        # 3. æœ€ç»ˆå¥–åŠ±å¯¹æ¯”ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
        reward_data = successful_df.dropna(subset=['final_reward'])
        if len(reward_data) > 0:
            sns.boxplot(data=reward_data, x='config_name', y='final_reward', ax=axes[1,0])
            axes[1,0].set_title('Final Reward Comparison')
            axes[1,0].set_ylabel('Final Reward')
            axes[1,0].tick_params(axis='x', rotation=45)
        else:
            axes[1,0].text(0.5, 0.5, 'No reward data available', 
                          ha='center', va='center', transform=axes[1,0].transAxes)
            axes[1,0].set_title('Final Reward Comparison')
        
        # 4. ç»Ÿè®¡æ‘˜è¦è¡¨
        axes[1,1].axis('tight')
        axes[1,1].axis('off')
        
        # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
        table_data = []
        headers = ['Metric', 'With Sensors', 'Baseline']
        
        # æˆåŠŸç‡
        table_data.append([
            'Success Rate (%)',
            f"{stats['with_sensors']['success_rate']*100:.1f}",
            f"{stats['baseline']['success_rate']*100:.1f}"
        ])
        
        # å¹³å‡è®­ç»ƒæ—¶é—´
        sensor_time = stats['with_sensors']['avg_training_time']
        baseline_time = stats['baseline']['avg_training_time']
        if sensor_time and baseline_time:
            table_data.append([
                'Avg Training Time (s)',
                f"{sensor_time:.1f} Â± {stats['with_sensors']['std_training_time']:.1f}",
                f"{baseline_time:.1f} Â± {stats['baseline']['std_training_time']:.1f}"
            ])
            
            # è®¡ç®—ç›¸å¯¹å·®å¼‚
            time_diff = ((sensor_time - baseline_time) / baseline_time) * 100
            table_data.append([
                'Time Difference (%)',
                f"{time_diff:+.1f}",
                "baseline"
            ])
        
        table = axes[1,1].table(cellText=table_data, colLabels=headers,
                               cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.5)
        axes[1,1].set_title('Statistical Summary')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_file = self.experiment_dir / "comparison_plots.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ [SAVE] Plots saved to {plot_file}")
        
        plt.show()
    
    def generate_report(self, stats: Dict):
        """ç”Ÿæˆè¯¦ç»†çš„å®éªŒæŠ¥å‘Š"""
        print(f"ğŸ“‹ [REPORT] Generating experiment report...")
        
        report_file = self.experiment_dir / "experiment_report.md"
        
        with open(report_file, 'w') as f:
            f.write(f"# ä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒæŠ¥å‘Š\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**å®éªŒç›®å½•**: `{self.experiment_dir}`\n\n")
            
            f.write(f"## å®éªŒé…ç½®\n\n")
            f.write(f"- **éšæœºç§å­æ•°é‡**: {self.args.num_seeds}\n")
            f.write(f"- **æœ€å¤§è¿­ä»£æ¬¡æ•°**: {self.args.max_iterations}\n")
            f.write(f"- **ç¯å¢ƒæ•°é‡**: {self.args.num_envs}\n")
            f.write(f"- **å¯¹æ¯”ç‰ˆæœ¬**:\n")
            for config_name, config in self.configs.items():
                f.write(f"  - {config['name']}: `{config['script']}`\n")
            
            f.write(f"\n## å®éªŒç»“æœ\n\n")
            
            for config_name, stat in stats.items():
                config = self.configs[config_name]
                f.write(f"### {config['name']}\n\n")
                f.write(f"- **æ€»è¿è¡Œæ¬¡æ•°**: {stat['total_runs']}\n")
                f.write(f"- **æˆåŠŸè¿è¡Œæ¬¡æ•°**: {stat['successful_runs']}\n")
                f.write(f"- **æˆåŠŸç‡**: {stat['success_rate']*100:.1f}%\n")
                
                if stat['avg_training_time']:
                    f.write(f"- **å¹³å‡è®­ç»ƒæ—¶é—´**: {stat['avg_training_time']:.1f} Â± {stat['std_training_time']:.1f} ç§’\n")
                
                if stat['avg_final_reward']:
                    f.write(f"- **å¹³å‡æœ€ç»ˆå¥–åŠ±**: {stat['avg_final_reward']:.3f} Â± {stat['std_final_reward']:.3f}\n")
                
                f.write(f"\n")
            
            # æ·»åŠ ç»“è®º
            f.write(f"## ç»“è®º\n\n")
            
            sensor_stats = stats['with_sensors']
            baseline_stats = stats['baseline']
            
            if sensor_stats['avg_training_time'] and baseline_stats['avg_training_time']:
                time_diff = ((sensor_stats['avg_training_time'] - baseline_stats['avg_training_time']) 
                           / baseline_stats['avg_training_time']) * 100
                
                if time_diff > 5:
                    f.write(f"- ğŸŒ ä¼ æ„Ÿå™¨ç‰ˆæœ¬çš„è®­ç»ƒæ—¶é—´æ¯”åŸºçº¿ç‰ˆæœ¬é•¿ {time_diff:.1f}%ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºé¢å¤–çš„ä¼ æ„Ÿå™¨æ•°æ®å¤„ç†å¼€é”€\n")
                elif time_diff < -5:
                    f.write(f"- ğŸš€ ä¼ æ„Ÿå™¨ç‰ˆæœ¬çš„è®­ç»ƒæ—¶é—´æ¯”åŸºçº¿ç‰ˆæœ¬çŸ­ {abs(time_diff):.1f}%ï¼Œä¼ æ„Ÿå™¨ä¿¡æ¯å¯èƒ½æœ‰åŠ©äºæ›´å¿«æ”¶æ•›\n")
                else:
                    f.write(f"- âš–ï¸ ä¸¤ä¸ªç‰ˆæœ¬çš„è®­ç»ƒæ—¶é—´åŸºæœ¬ç›¸å½“ï¼ˆå·®å¼‚ {time_diff:+.1f}%ï¼‰\n")
            
            success_diff = (sensor_stats['success_rate'] - baseline_stats['success_rate']) * 100
            if success_diff > 10:
                f.write(f"- âœ… ä¼ æ„Ÿå™¨ç‰ˆæœ¬çš„æˆåŠŸç‡æ˜æ˜¾æ›´é«˜ï¼ˆ+{success_diff:.1f}%ï¼‰ï¼Œä¼ æ„Ÿå™¨ä¿¡æ¯æ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§\n")
            elif success_diff < -10:
                f.write(f"- âŒ ä¼ æ„Ÿå™¨ç‰ˆæœ¬çš„æˆåŠŸç‡è¾ƒä½ï¼ˆ{success_diff:.1f}%ï¼‰ï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜\n")
            else:
                f.write(f"- ğŸ“Š ä¸¤ä¸ªç‰ˆæœ¬çš„æˆåŠŸç‡ç›¸å½“ï¼ˆå·®å¼‚ {success_diff:+.1f}%ï¼‰\n")
        
        print(f"ğŸ“„ [SAVE] Report saved to {report_file}")
    
    def run_complete_experiment(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”å®éªŒ"""
        try:
            # è¿è¡Œæ‰€æœ‰å®éªŒ
            self.run_all_experiments()
            
            # åˆ†æç»“æœ
            df, stats = self.analyze_results()
            
            # ç”Ÿæˆå¯è§†åŒ–
            self.generate_visualizations(df, stats)
            
            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report(stats)
            
            print(f"\nğŸ‰ [COMPLETE] Experiment completed successfully!")
            print(f"ğŸ“ Results saved in: {self.experiment_dir}")
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ [INTERRUPTED] Experiment interrupted by user")
            print(f"ğŸ“ Partial results saved in: {self.experiment_dir}")
        
        except Exception as e:
            print(f"\nğŸ’¥ [ERROR] Experiment failed: {e}")
            print(f"ğŸ“ Partial results saved in: {self.experiment_dir}")
            raise


def main():
    parser = argparse.ArgumentParser(description="ä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒ")
    parser.add_argument("--num_seeds", type=int, default=3, help="éšæœºç§å­æ•°é‡")
    parser.add_argument("--max_iterations", type=int, default=1000, help="æœ€å¤§è®­ç»ƒè¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num_envs", type=int, default=32, help="ç¯å¢ƒæ•°é‡")
    parser.add_argument("--disable_fabric", action="store_true", help="ç¦ç”¨fabric")
    parser.add_argument("--timeout", type=int, default=7200, help="å•ä¸ªè®­ç»ƒçš„è¶…æ—¶æ—¶é—´(ç§’)")
    parser.add_argument("--cleanup_wait", type=int, default=60, help="è®­ç»ƒé—´éš”çš„æ¸…ç†ç­‰å¾…æ—¶é—´(ç§’)")
    parser.add_argument("--disable_cleanup", action="store_true", help="ç¦ç”¨è‡ªåŠ¨è¿›ç¨‹æ¸…ç†(ä¸æ¨è)")
    parser.add_argument("--gpu_memory_threshold", type=int, default=80, help="GPUå†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼ç™¾åˆ†æ¯”ï¼Œè¶…è¿‡æ­¤å€¼å°†ç­‰å¾…")
    parser.add_argument("--status_interval", type=int, default=5, help="çŠ¶æ€æ˜¾ç¤ºé—´éš”(æ¯Næ¬¡è¿­ä»£æ˜¾ç¤ºä¸€æ¬¡)")
    parser.add_argument("--disable_realtime_monitoring", action="store_true", help="ç¦ç”¨å®æ—¶ç›‘æ§ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.status_interval <= 0:
        print("âŒ [ERROR] Status interval must be positive")
        exit(1)
    
    # å¦‚æœç¦ç”¨æ¸…ç†ï¼Œç»™å‡ºè­¦å‘Š
    if args.disable_cleanup:
        print("âš ï¸ [WARNING] Process cleanup is disabled!")
        print("âš ï¸ This may cause resource conflicts between training runs")
        user_confirm = input("ğŸ¤” Are you sure you want to continue? (y/n): ").lower().strip()
        if user_confirm not in ['y', 'yes']:
            print("âŒ Experiment cancelled")
            exit(0)
    
    # å¦‚æœç¦ç”¨å®æ—¶ç›‘æ§ï¼Œç»™å‡ºæç¤º
    if args.disable_realtime_monitoring:
        print("ğŸ“Š [INFO] Real-time monitoring disabled, using traditional mode")
    else:
        print(f"ğŸ“Š [INFO] Real-time monitoring enabled (status every {args.status_interval} iterations)")
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = SensorComparisonExperiment(args)
    experiment.run_complete_experiment()


if __name__ == "__main__":
    main() 