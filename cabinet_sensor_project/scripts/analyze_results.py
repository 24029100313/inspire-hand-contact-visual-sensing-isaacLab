#!/usr/bin/env python3
"""
åˆ†æä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒç»“æœ

åˆ†æsensor_comparison_20250709_234139å®éªŒçš„ç»“æœï¼Œæå–å…³é”®æŒ‡æ ‡ï¼š
1. ä¸¤ä¸ªç‰ˆæœ¬çš„æˆåŠŸç‡å¯¹æ¯”
2. å­¦ä¹ æ›²çº¿å’Œå¥–åŠ±å˜åŒ–
3. ç¬¬ä¸€æ¬¡å­¦ä¼šå¼€æŠ½å±‰çš„æ—¶é—´
4. è¯¦ç»†çš„æ€§èƒ½åˆ†æ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import re
from pathlib import Path

class ExperimentAnalyzer:
    def __init__(self, results_file):
        self.results_file = results_file
        self.results = None
        self.training_data = {}
        
    def load_results(self):
        """åŠ è½½å®éªŒç»“æœ"""
        print(f"ğŸ“‚ [LOAD] Loading experiment results from {self.results_file}")
        
        try:
            with open(self.results_file, 'r') as f:
                self.results = json.load(f)
            print(f"âœ… [SUCCESS] Results loaded successfully")
            
            # æ‰“å°åŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“Š [INFO] Available configurations: {list(self.results.keys())}")
            for config_name, results in self.results.items():
                print(f"  - {config_name}: {len(results)} runs")
                
        except Exception as e:
            print(f"âŒ [ERROR] Failed to load results: {e}")
            return False
            
        return True
    
    def parse_training_logs(self):
        """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–å­¦ä¹ æ›²çº¿æ•°æ®"""
        print("ğŸ“Š [PARSE] Parsing training logs...")
        
        for config_name, results in self.results.items():
            self.training_data[config_name] = []
            
            for run_idx, result in enumerate(results):
                print(f"ğŸ” [PARSE] Processing {config_name} run {run_idx + 1}")
                
                # è§£æè®­ç»ƒè¾“å‡ºæ—¥å¿—
                training_log = result.get('stdout', '')
                
                # æå–è®­ç»ƒæŒ‡æ ‡
                iterations = []
                rewards = []
                losses = []
                times = []
                
                # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é…è®­ç»ƒè¾“å‡º
                iteration_pattern = r'Learning iteration (\d+)/\d+'
                reward_pattern = r'Mean reward: ([\d.-]+)'
                loss_pattern = r'Mean entropy loss: ([-+]?\d*\.?\d+)'
                time_pattern = r'Time elapsed: (\d{2}:\d{2}:\d{2})'
                
                lines = training_log.split('\n')
                current_iteration = None
                
                for line in lines:
                    # æŸ¥æ‰¾è¿­ä»£æ•°
                    iter_match = re.search(iteration_pattern, line)
                    if iter_match:
                        current_iteration = int(iter_match.group(1))
                    
                    # æŸ¥æ‰¾å¥–åŠ±
                    reward_match = re.search(reward_pattern, line)
                    if reward_match and current_iteration:
                        reward = float(reward_match.group(1))
                        iterations.append(current_iteration)
                        rewards.append(reward)
                    
                    # æŸ¥æ‰¾æŸå¤±
                    loss_match = re.search(loss_pattern, line)
                    if loss_match and current_iteration and len(losses) < len(rewards):
                        loss = float(loss_match.group(1))
                        losses.append(loss)
                
                # å­˜å‚¨è§£æçš„æ•°æ®
                run_data = {
                    'seed': result.get('seed', run_idx),
                    'success': result.get('success', False),
                    'training_time': result.get('training_time', 0),
                    'iterations': iterations,
                    'rewards': rewards,
                    'losses': losses,
                    'final_iteration': result.get('final_iteration', 0),
                    'final_metrics': result.get('final_metrics', {})
                }
                
                self.training_data[config_name].append(run_data)
                
                print(f"  âœ… Found {len(iterations)} training points for seed {run_data['seed']}")
    
    def analyze_success_rates(self):
        """åˆ†ææˆåŠŸç‡"""
        print("ğŸ“ˆ [ANALYSIS] Analyzing success rates...")
        
        success_analysis = {}
        
        for config_name, results in self.results.items():
            total_runs = len(results)
            successful_runs = sum(1 for r in results if r.get('success', False))
            success_rate = successful_runs / total_runs if total_runs > 0 else 0
            
            success_analysis[config_name] = {
                'total_runs': total_runs,
                'successful_runs': successful_runs,
                'success_rate': success_rate,
                'success_percentage': success_rate * 100
            }
            
            print(f"ğŸ¯ [RESULT] {config_name}:")
            print(f"   æ€»è¿è¡Œæ¬¡æ•°: {total_runs}")
            print(f"   æˆåŠŸæ¬¡æ•°: {successful_runs}")
            print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        
        return success_analysis
    
    def analyze_learning_performance(self):
        """åˆ†æå­¦ä¹ æ€§èƒ½"""
        print("ğŸ“š [ANALYSIS] Analyzing learning performance...")
        
        performance_analysis = {}
        
        for config_name, runs in self.training_data.items():
            successful_runs = [run for run in runs if run['success']]
            
            if not successful_runs:
                print(f"âš ï¸ [WARNING] No successful runs for {config_name}")
                continue
            
            # è®¡ç®—å¹³å‡å­¦ä¹ æ›²çº¿
            all_rewards = []
            all_iterations = []
            
            for run in successful_runs:
                if run['rewards']:
                    all_rewards.extend(run['rewards'])
                    all_iterations.extend(run['iterations'])
            
            if all_rewards:
                # è®¡ç®—é¦–æ¬¡è¾¾åˆ°è‰¯å¥½æ€§èƒ½çš„æ—¶é—´
                good_performance_threshold = 80  # å¥–åŠ±é˜ˆå€¼
                first_success_iterations = []
                
                for run in successful_runs:
                    rewards = run['rewards']
                    iterations = run['iterations']
                    
                    for i, reward in enumerate(rewards):
                        if reward >= good_performance_threshold:
                            first_success_iterations.append(iterations[i])
                            break
                
                avg_first_success = np.mean(first_success_iterations) if first_success_iterations else None
                
                performance_analysis[config_name] = {
                    'avg_final_reward': np.mean([run['rewards'][-1] for run in successful_runs if run['rewards']]),
                    'std_final_reward': np.std([run['rewards'][-1] for run in successful_runs if run['rewards']]),
                    'avg_training_time': np.mean([run['training_time'] for run in successful_runs]),
                    'std_training_time': np.std([run['training_time'] for run in successful_runs]),
                    'avg_first_success_iteration': avg_first_success,
                    'num_successful_learning': len(first_success_iterations),
                    'all_rewards': all_rewards,
                    'all_iterations': all_iterations
                }
                
                print(f"ğŸš€ [RESULT] {config_name}:")
                print(f"   å¹³å‡æœ€ç»ˆå¥–åŠ±: {performance_analysis[config_name]['avg_final_reward']:.2f} Â± {performance_analysis[config_name]['std_final_reward']:.2f}")
                print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {performance_analysis[config_name]['avg_training_time']:.1f} Â± {performance_analysis[config_name]['std_training_time']:.1f}ç§’")
                if avg_first_success:
                    print(f"   é¦–æ¬¡å­¦ä¼šæ—¶é—´: å¹³å‡ç¬¬{avg_first_success:.0f}æ¬¡è¿­ä»£")
                print(f"   æˆåŠŸå­¦ä¼šæ¬¡æ•°: {len(first_success_iterations)}/{len(successful_runs)}")
        
        return performance_analysis
    
    def create_visualizations(self, success_analysis, performance_analysis):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š [VISUALIZATION] Creating comparison plots...")
        
        # è®¾ç½®matplotlibå­—ä½“ä»¥æ”¯æŒä¸­æ–‡
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Sensor vs Baseline Comparison Analysis', fontsize=16, fontweight='bold')
        
        # 1. æˆåŠŸç‡å¯¹æ¯”
        config_names = list(success_analysis.keys())
        success_rates = [success_analysis[name]['success_percentage'] for name in config_names]
        
        colors = ['#2E86AB', '#A23B72']  # è“è‰²forä¼ æ„Ÿå™¨, ç´«è‰²foråŸºçº¿
        bars = axes[0,0].bar(config_names, success_rates, color=colors[:len(config_names)])
        axes[0,0].set_title('Training Success Rate Comparison')
        axes[0,0].set_ylabel('Success Rate (%)')
        axes[0,0].set_ylim(0, 100)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate in zip(bars, success_rates):
            axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                          f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # 2. æœ€ç»ˆå¥–åŠ±å¯¹æ¯”
        if len(performance_analysis) >= 2:
            config_names_perf = list(performance_analysis.keys())
            final_rewards = [performance_analysis[name]['avg_final_reward'] for name in config_names_perf]
            final_rewards_std = [performance_analysis[name]['std_final_reward'] for name in config_names_perf]
            
            bars2 = axes[0,1].bar(config_names_perf, final_rewards, 
                                 yerr=final_rewards_std, capsize=5, color=colors[:len(config_names_perf)])
            axes[0,1].set_title('Final Reward Comparison')
            axes[0,1].set_ylabel('Mean Final Reward')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, reward, std in zip(bars2, final_rewards, final_rewards_std):
                axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.5,
                              f'{reward:.1f}Â±{std:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. å­¦ä¹ æ›²çº¿
        if performance_analysis:
            for i, (config_name, data) in enumerate(performance_analysis.items()):
                if data['all_rewards'] and data['all_iterations']:
                    # åˆ›å»ºå¹³æ»‘çš„å­¦ä¹ æ›²çº¿
                    iterations = np.array(data['all_iterations'])
                    rewards = np.array(data['all_rewards'])
                    
                    # æŒ‰è¿­ä»£æ¬¡æ•°æ’åº
                    sorted_indices = np.argsort(iterations)
                    iterations_sorted = iterations[sorted_indices]
                    rewards_sorted = rewards[sorted_indices]
                    
                    # ä½¿ç”¨æ»‘åŠ¨å¹³å‡å¹³æ»‘æ›²çº¿
                    window_size = max(1, len(rewards_sorted) // 50)
                    if len(rewards_sorted) > window_size:
                        smoothed_rewards = pd.Series(rewards_sorted).rolling(window=window_size, min_periods=1).mean()
                        axes[1,0].plot(iterations_sorted, smoothed_rewards, 
                                      label=config_name, color=colors[i], linewidth=2)
            
            axes[1,0].set_title('Learning Curves (Smoothed)')
            axes[1,0].set_xlabel('Training Iteration')
            axes[1,0].set_ylabel('Mean Reward')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
        
        # 4. å­¦ä¹ é€Ÿåº¦å¯¹æ¯”
        if len(performance_analysis) >= 2:
            config_names_speed = list(performance_analysis.keys())
            first_success_iters = [performance_analysis[name]['avg_first_success_iteration'] 
                                  for name in config_names_speed 
                                  if performance_analysis[name]['avg_first_success_iteration'] is not None]
            valid_names = [name for name in config_names_speed 
                          if performance_analysis[name]['avg_first_success_iteration'] is not None]
            
            if first_success_iters:
                bars3 = axes[1,1].bar(valid_names, first_success_iters, color=colors[:len(valid_names)])
                axes[1,1].set_title('Time to First Success')
                axes[1,1].set_ylabel('Iterations to Reach Good Performance')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, iters in zip(bars3, first_success_iters):
                    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                                  f'{iters:.0f}', ha='center', va='bottom', fontweight='bold')
            else:
                axes[1,1].text(0.5, 0.5, 'No successful learning data', 
                              ha='center', va='center', transform=axes[1,1].transAxes)
                axes[1,1].set_title('Time to First Success')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_dir = Path("experiment_analysis")
        output_dir.mkdir(exist_ok=True)
        
        plot_file = output_dir / "sensor_comparison_analysis.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ [SAVE] Analysis plots saved to {plot_file}")
        
        plt.show()
    
    def generate_report(self, success_analysis, performance_analysis):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ [REPORT] Generating analysis report...")
        
        output_dir = Path("experiment_analysis")
        output_dir.mkdir(exist_ok=True)
        
        report_file = output_dir / "sensor_comparison_report.md"
        
        with open(report_file, 'w') as f:
            f.write("# ä¼ æ„Ÿå™¨å¯¹æ¯”å®éªŒåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**å®éªŒæ•°æ®**: sensor_comparison_20250709_234139\n\n")
            
            # æˆåŠŸç‡åˆ†æ
            f.write("## ğŸ¯ æˆåŠŸç‡åˆ†æ\n\n")
            for config_name, data in success_analysis.items():
                f.write(f"### {config_name}\n")
                f.write(f"- **æ€»è¿è¡Œæ¬¡æ•°**: {data['total_runs']}\n")
                f.write(f"- **æˆåŠŸè¿è¡Œæ¬¡æ•°**: {data['successful_runs']}\n")
                f.write(f"- **æˆåŠŸç‡**: {data['success_percentage']:.1f}%\n\n")
            
            # æ€§èƒ½åˆ†æ
            f.write("## ğŸ“ˆ æ€§èƒ½åˆ†æ\n\n")
            for config_name, data in performance_analysis.items():
                f.write(f"### {config_name}\n")
                f.write(f"- **å¹³å‡æœ€ç»ˆå¥–åŠ±**: {data['avg_final_reward']:.2f} Â± {data['std_final_reward']:.2f}\n")
                f.write(f"- **å¹³å‡è®­ç»ƒæ—¶é—´**: {data['avg_training_time']:.1f} Â± {data['std_training_time']:.1f} ç§’\n")
                if data['avg_first_success_iteration']:
                    f.write(f"- **é¦–æ¬¡å­¦ä¼šæ—¶é—´**: å¹³å‡ç¬¬ {data['avg_first_success_iteration']:.0f} æ¬¡è¿­ä»£\n")
                f.write(f"- **æˆåŠŸå­¦ä¼šç‡**: {data['num_successful_learning']}/{len(self.training_data[config_name])} ({data['num_successful_learning']/len(self.training_data[config_name])*100:.1f}%)\n\n")
            
            # å¯¹æ¯”ç»“è®º
            f.write("## ğŸ” å¯¹æ¯”ç»“è®º\n\n")
            if len(success_analysis) >= 2:
                configs = list(success_analysis.keys())
                sensor_config = next((c for c in configs if 'sensor' in c.lower()), configs[0])
                baseline_config = next((c for c in configs if 'baseline' in c.lower()), configs[1] if len(configs) > 1 else configs[0])
                
                if sensor_config in success_analysis and baseline_config in success_analysis:
                    sensor_success = success_analysis[sensor_config]['success_percentage']
                    baseline_success = success_analysis[baseline_config]['success_percentage']
                    
                    f.write(f"### æˆåŠŸç‡å¯¹æ¯”\n")
                    f.write(f"- **ä¼ æ„Ÿå™¨ç‰ˆæœ¬**: {sensor_success:.1f}%\n")
                    f.write(f"- **åŸºçº¿ç‰ˆæœ¬**: {baseline_success:.1f}%\n")
                    f.write(f"- **å·®å¼‚**: {sensor_success - baseline_success:+.1f}%\n\n")
                    
                    if sensor_config in performance_analysis and baseline_config in performance_analysis:
                        sensor_perf = performance_analysis[sensor_config]
                        baseline_perf = performance_analysis[baseline_config]
                        
                        f.write(f"### æ€§èƒ½å¯¹æ¯”\n")
                        f.write(f"- **ä¼ æ„Ÿå™¨ç‰ˆæœ¬æœ€ç»ˆå¥–åŠ±**: {sensor_perf['avg_final_reward']:.2f}\n")
                        f.write(f"- **åŸºçº¿ç‰ˆæœ¬æœ€ç»ˆå¥–åŠ±**: {baseline_perf['avg_final_reward']:.2f}\n")
                        f.write(f"- **å¥–åŠ±å·®å¼‚**: {sensor_perf['avg_final_reward'] - baseline_perf['avg_final_reward']:+.2f}\n\n")
                        
                        if sensor_perf['avg_first_success_iteration'] and baseline_perf['avg_first_success_iteration']:
                            f.write(f"### å­¦ä¹ é€Ÿåº¦å¯¹æ¯”\n")
                            f.write(f"- **ä¼ æ„Ÿå™¨ç‰ˆæœ¬é¦–æ¬¡å­¦ä¼š**: ç¬¬ {sensor_perf['avg_first_success_iteration']:.0f} æ¬¡è¿­ä»£\n")
                            f.write(f"- **åŸºçº¿ç‰ˆæœ¬é¦–æ¬¡å­¦ä¼š**: ç¬¬ {baseline_perf['avg_first_success_iteration']:.0f} æ¬¡è¿­ä»£\n")
                            f.write(f"- **å­¦ä¹ é€Ÿåº¦å·®å¼‚**: {baseline_perf['avg_first_success_iteration'] - sensor_perf['avg_first_success_iteration']:+.0f} è¿­ä»£\n\n")
            
            # æ€»ç»“
            f.write("## ğŸ“ æ€»ç»“\n\n")
            f.write("åŸºäºä»¥ä¸Šåˆ†æï¼Œå¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š\n\n")
            f.write("1. **ç¨³å®šæ€§**: æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬çš„è®­ç»ƒæˆåŠŸç‡ï¼Œè¯„ä¼°å“ªä¸ªç‰ˆæœ¬æ›´ç¨³å®š\n")
            f.write("2. **æ€§èƒ½**: æ¯”è¾ƒæœ€ç»ˆå¥–åŠ±å€¼ï¼Œè¯„ä¼°å“ªä¸ªç‰ˆæœ¬èƒ½è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½\n")
            f.write("3. **å­¦ä¹ æ•ˆç‡**: æ¯”è¾ƒé¦–æ¬¡å­¦ä¼šçš„æ—¶é—´ï¼Œè¯„ä¼°å“ªä¸ªç‰ˆæœ¬å­¦ä¹ æ›´å¿«\n")
            f.write("4. **è®­ç»ƒæ—¶é—´**: æ¯”è¾ƒæ€»è®­ç»ƒæ—¶é—´ï¼Œè¯„ä¼°è®¡ç®—æ•ˆç‡\n\n")
            
        print(f"ğŸ“„ [SAVE] Analysis report saved to {report_file}")
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        if not self.load_results():
            return
        
        self.parse_training_logs()
        success_analysis = self.analyze_success_rates()
        performance_analysis = self.analyze_learning_performance()
        
        self.create_visualizations(success_analysis, performance_analysis)
        self.generate_report(success_analysis, performance_analysis)
        
        print("\nğŸ‰ [COMPLETE] Analysis completed successfully!")
        print(f"ğŸ“ Results saved in experiment_analysis/ directory")

def main():
    # å®éªŒç»“æœæ–‡ä»¶è·¯å¾„
    results_file = "experiments/sensor_comparison_20250709_234139/intermediate_results.json"
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = ExperimentAnalyzer(results_file)
    analyzer.run_analysis()

if __name__ == "__main__":
    main() 